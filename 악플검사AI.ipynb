{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865a0c34-def4-4988-8d46-3de90192caef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('shopping.txt', <http.client.HTTPMessage at 0x21a9ab2b250>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt', 'shopping.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb5a477-6a0d-4d43-ba70-c0035d57cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating                                             review\n",
      "0            5                                            배공빠르고 굿\n",
      "1            2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n",
      "2            5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...\n",
      "3            2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n",
      "4            5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ\n",
      "...        ...                                                ...\n",
      "199995       2                                    장마라그런가!!! 달지않아요\n",
      "199996       5  다이슨 케이스 구매했어요 다이슨 슈퍼소닉 드라이기 케이스 구매했어요가격 괜찮고 배송...\n",
      "199997       5                    로드샾에서 사는것보다 세배 저렴하네요 ㅜㅜ 자주이용할께요\n",
      "199998       5                                      넘이쁘고 쎄련되보이네요~\n",
      "199999       5   아직 사용해보지도않았고 다른 제품을 써본적이없어서 잘 모르겠지만 ㅎㅎ 배송은 빨랐습니다\n",
      "\n",
      "[200000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "raw= pd.read_table('shopping.txt',names=['rating','review'])\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9012f9-7fae-40cf-85d3-5d0209e61e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating                                             review  label\n",
      "0            5                                            배공빠르고 굿      1\n",
      "1            2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고      0\n",
      "2            5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...      1\n",
      "3            2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...      0\n",
      "4            5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ      1\n",
      "...        ...                                                ...    ...\n",
      "199995       2                                    장마라그런가!!! 달지않아요      0\n",
      "199996       5  다이슨 케이스 구매했어요 다이슨 슈퍼소닉 드라이기 케이스 구매했어요가격 괜찮고 배송...      1\n",
      "199997       5                    로드샾에서 사는것보다 세배 저렴하네요 ㅜㅜ 자주이용할께요      1\n",
      "199998       5                                      넘이쁘고 쎄련되보이네요~      1\n",
      "199999       5   아직 사용해보지도않았고 다른 제품을 써본적이없어서 잘 모르겠지만 ㅎㅎ 배송은 빨랐습니다      1\n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "raw['label']=np.where(raw['rating']>3,1,0)\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04253717-9adb-4e1f-8e5a-322295837a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating                                             review  label\n",
      "0            5                                            배공빠르고 굿      1\n",
      "1            2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고      0\n",
      "2            5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...      1\n",
      "3            2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...      0\n",
      "4            5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ      1\n",
      "...        ...                                                ...    ...\n",
      "199995       2                                    장마라그런가!!! 달지않아요      0\n",
      "199996       5  다이슨 케이스 구매했어요 다이슨 슈퍼소닉 드라이기 케이스 구매했어요가격 괜찮고 배송...      1\n",
      "199997       5                    로드샾에서 사는것보다 세배 저렴하네요 ㅜㅜ 자주이용할께요      1\n",
      "199998       5                                      넘이쁘고 쎄련되보이네요~      1\n",
      "199999       5   아직 사용해보지도않았고 다른 제품을 써본적이없어서 잘 모르겠지만 ㅎㅎ 배송은 빨랐습니다      1\n",
      "\n",
      "[199908 rows x 3 columns]\n",
      "['\\x02', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '£', '¥', '¨']\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "raw['review']=raw['review'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]','')\n",
    "#print(raw.isnull().sum())\n",
    "\n",
    "raw.drop_duplicates(subset=['review'],inplace=True)\n",
    "print(raw)\n",
    "\n",
    "# bag of words\n",
    "유니크문자=raw['review'].tolist()\n",
    "유니크문자=''.join(유니크문자)\n",
    "유니크문자=list(set(유니크문자))\n",
    "유니크문자.sort()\n",
    "print(유니크문자[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbe36ce-5aeb-4fc7-ad4b-70ba0ac45b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['배공빠르고 굿', '택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고', '아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 엉성하긴 하지만 편하고 가성비 최고예요.', '선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전화했더니 바로주신다했지만 배송도 누락되어있었네요.. 확인안하고 바로 선물했으면 큰일날뻔했네요..이렇게 배송이 오래걸렸으면 사는거 다시 생각했을거같아요 아쉽네요..', '민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ', '비추합니다 계란 뒤집을 때 완전 불편해요 ㅠㅠ 코팅도 묻어나고 보기엔 예쁘고 실용적으로 보였는데 생각보다 진짜 별로입니다.', '주문을 11월6에 시켰는데 11월16일에 배송이 왔네요 ㅎㅎㅎ 여기 회사측과는 전화도 안되고 아무런 연락을 받을수가 없으니 답답하신 분들은 다른곳에서 사시는거 추천드립니다', '넉넉한 길이로 주문했는데도 안 맞네요 별로예요', '보폴이 계속 때처럼 나오다가 지금은 안나네요~', '110인데 전문속옷브랜드 위생팬티105보다 작은듯해요. 불편해요. 밴딩부분이 다 신축성없는 일반실로 되어있어 빅사이즈임에도 빅사이즈같지않아요. 입고벗을때 편하게 밴딩부분이 늘어나고 입었을때도 밴딩이 잡아주어야하는데 말이죠.']\n",
      "[[42, 331, 93, 92, 6, 2, 305], [299, 42, 13, 2, 408, 227, 5, 17, 49, 2, 88, 578, 226, 2, 527, 18, 924, 18, 2, 138, 15, 71, 5, 2, 682, 115, 6, 13, 6], [8, 40, 20, 8, 3, 2, 118, 16, 2, 65, 138, 2, 20, 8, 14, 172, 107, 2, 80, 2, 21, 32, 45, 10, 3, 2, 5, 13, 119, 18, 2, 70, 231, 56, 12, 7, 4, 2, 118, 146, 139, 5, 2, 98, 161, 2, 408, 163, 11, 219, 2, 11, 16, 27, 2, 96, 11, 6, 2, 13, 163, 67, 2, 268, 6, 134, 3, 4], [167, 102, 49, 51, 26, 2, 187, 35, 2, 136, 8, 14, 2, 79, 179, 45, 10, 127, 2, 11, 9, 2, 57, 50, 5, 105, 9, 19, 2, 277, 36, 686, 27, 2, 183, 14, 2, 192, 546, 45, 46, 12, 7, 4, 2, 79, 190, 45, 80, 12, 2, 118, 26, 40, 122, 7, 45, 16, 27, 2, 42, 48, 15, 2, 375, 328, 78, 10, 31, 105, 17, 3, 4, 4, 2, 256, 60, 38, 11, 6, 2, 118, 26, 2, 167, 102, 45, 51, 62, 2, 347, 84, 236, 720, 45, 17, 3, 4, 4, 5, 275, 33, 2, 42, 48, 5, 2, 99, 116, 152, 212, 51, 62, 2, 22, 9, 43, 2, 7, 47, 2, 91, 109, 45, 55, 43, 54, 8, 3, 2, 8, 265, 17, 3, 4, 4], [391, 154, 124, 57, 2, 134, 253, 3, 4, 2, 582, 2, 230, 311, 5, 9, 2, 43, 9, 2, 49, 15, 26, 15, 2, 22, 49, 78, 17, 3, 2, 86, 86], [67, 168, 68, 12, 7, 2, 250, 442, 2, 448, 226, 55, 2, 137, 2, 228, 79, 2, 123, 96, 24, 3, 2, 58, 58, 2, 385, 612, 15, 2, 468, 10, 28, 6, 2, 30, 23, 315, 2, 134, 218, 6, 2, 128, 49, 175, 51, 26, 2, 30, 296, 9, 19, 2, 91, 109, 30, 7, 2, 97, 171, 2, 125, 26, 56, 12, 7, 4], [40, 83, 55, 2, 148, 148, 422, 363, 18, 2, 47, 394, 9, 19, 2, 148, 148, 422, 148, 363, 84, 18, 2, 42, 48, 5, 2, 156, 17, 3, 2, 86, 86, 86, 2, 72, 23, 2, 336, 22, 822, 158, 9, 2, 79, 190, 15, 2, 38, 78, 6, 2, 8, 29, 194, 2, 232, 328, 55, 2, 136, 55, 90, 13, 2, 71, 51, 12, 2, 387, 387, 11, 122, 2, 95, 59, 25, 2, 7, 130, 349, 18, 14, 2, 22, 47, 9, 43, 2, 168, 234, 104, 259, 12, 7], [371, 371, 39, 2, 223, 5, 26, 2, 40, 83, 45, 9, 19, 15, 2, 38, 2, 177, 17, 3, 2, 125, 26, 134, 3], [30, 856, 5, 2, 250, 197, 2, 137, 176, 337, 2, 28, 99, 7, 13, 2, 16, 161, 25, 2, 38, 28, 17, 3, 34], [148, 148, 147, 60, 19, 2, 79, 83, 197, 326, 406, 616, 104, 2, 270, 91, 604, 233, 148, 147, 244, 30, 7, 2, 135, 25, 166, 24, 3, 4, 2, 123, 96, 24, 3, 4, 2, 733, 698, 69, 95, 5, 2, 7, 2, 122, 536, 163, 71, 9, 2, 84, 162, 128, 26, 2, 78, 10, 31, 10, 2, 1039, 22, 5, 144, 302, 18, 15, 2, 1039, 22, 5, 144, 54, 16, 89, 8, 3, 4, 2, 56, 6, 520, 55, 137, 2, 96, 11, 33, 2, 733, 698, 69, 95, 5, 2, 279, 10, 28, 6, 2, 56, 105, 55, 137, 15, 2, 733, 698, 5, 2, 311, 8, 40, 10, 127, 11, 9, 19, 2, 138, 5, 357, 4]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True,oov_token='<OOV>') # char_level=True 글자단위를 정수 false는 단어단위를 정수 oov_token 처음보는 단어는 어떻게 치환할지\n",
    "\n",
    "문자리스트 = raw['review'].tolist() # [문장1 , 문장2, ...]\n",
    "tokenizer.fit_on_texts(문자리스트)\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "print(문자리스트[0:10])\n",
    "\n",
    "train_seq=tokenizer.texts_to_sequences(문자리스트)\n",
    "print(train_seq[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0de710-ec51-4489-9093-3193d64f9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y=np.array(raw['label'].tolist())\n",
    "print(Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ec4c47-eef9-40ac-9e8e-503f3b0afeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                             review  label  length\n",
      "0       5                                            배공빠르고 굿      1       7\n",
      "1       2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고      0      29\n",
      "2       5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...      1      68\n",
      "3       2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...      0     136\n",
      "4       5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ      1      33\n",
      "              rating          label         length\n",
      "count  199908.000000  199908.000000  199908.000000\n",
      "mean        3.225909       0.499995      39.699402\n",
      "std         1.645431       0.500001      29.168119\n",
      "min         1.000000       0.000000       3.000000\n",
      "25%         2.000000       0.000000      17.000000\n",
      "50%         2.000000       0.000000      29.000000\n",
      "75%         5.000000       1.000000      55.000000\n",
      "max         5.000000       1.000000     140.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "189043"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 글자수 제한\n",
    "raw['length']=raw['review'].str.len()\n",
    "\n",
    "print(raw.head())\n",
    "print(raw.describe())\n",
    "\n",
    "raw['length'][raw['length']<100].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf25e677-4b43-4d92-a60d-ad372354edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159926\n",
      "39982\n"
     ]
    }
   ],
   "source": [
    "# 모든 문장 길이 100자로 제한하기\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X=pad_sequences(train_seq,maxlen=100)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainX,valX,trainY,valY=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "print(len(trainX))\n",
    "print(len(valX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce8635b-dac5-44cc-ab38-10521b0da834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 - 153s - loss: 0.3206 - acc: 0.8711 - val_loss: 0.2914 - val_acc: 0.8862 - 153s/epoch - 122ms/step\n",
      "Epoch 2/5\n",
      "1250/1250 - 151s - loss: 0.2603 - acc: 0.9018 - val_loss: 0.2466 - val_acc: 0.9080 - 151s/epoch - 121ms/step\n",
      "Epoch 3/5\n",
      "1250/1250 - 150s - loss: 0.2374 - acc: 0.9118 - val_loss: 0.2353 - val_acc: 0.9116 - 150s/epoch - 120ms/step\n",
      "Epoch 4/5\n",
      "1250/1250 - 152s - loss: 0.2244 - acc: 0.9174 - val_loss: 0.2287 - val_acc: 0.9145 - 152s/epoch - 122ms/step\n",
      "Epoch 5/5\n",
      "1250/1250 - 150s - loss: 0.2152 - acc: 0.9217 - val_loss: 0.2270 - val_acc: 0.9150 - 150s/epoch - 120ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          56656     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         74240     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 262,609\n",
      "Trainable params: 262,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model=tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index)+1,16), # 16개의 0~1사이의 무작위 숫\n",
    "    tf.keras.layers.LSTM(128,return_sequences=True), # 하나의 데이터 input_shape/ activation function 안넣어도 됨.\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc']) # 원핫 인코딩이면 sparse 안써도 됨.\n",
    "model.fit(trainX,trainY,validation_data=(valX,valY),epochs=5,batch_size=2**7,verbose=2) #64개 데이터 학습한 후에 w 값 업데이트 일어나게 하고 싶다 / LSTM epoch 많이 해야됨 / 'verbose=2' 파라미터는 학습중 출력되는 것좀 줄이는 것\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f70d5d-be58-46a3-81b6-39b0d1ad229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a952a67-88dd-4e84-af59-165e1214d2aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/data/tagset/mecab.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m utils\u001b[38;5;241m.\u001b[39minstallpath)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[1;32m----> 2\u001b[0m mecab\u001b[38;5;241m=\u001b[39m\u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\konlpy\\tag\\_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe MeCab dictionary does not exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab=Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ab3f-8bf3-4f87-bdfc-8703eb737781",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['는','은','다','을','를']\n",
    "raw['tokenized']=raw['reviews'].apply(mecab.morphs)\n",
    "raw['tokenized']=raw['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "x데이터 = raw['tokenized'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb18fd3-b57e-44bc-9c20-63b225c49950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#띄어쓰기 맞춤법 고치\n",
    "from hanspell import spell_checker\n",
    "hangul = \"마춤법 검사 하면 문장이 깔끔해지구 학습이 잘됀다\"\n",
    "hangul2 = spell_checker.check(hangul)\n",
    "print(hangul2.checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2598ea-7019-40c7-b57d-06a4c5e75107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적게 등장하는 단어 삭제\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True, oov_token=\"<OOV>\")  \n",
    "문자리스트 = raw['reviews'].tolist()\n",
    "print(문자리스트[0:5])\n",
    "\n",
    "tokenizer.fit_on_texts(문자리스트)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6ce7c-93db-47c5-84c8-cb71819bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    " tokenizer = Tokenizer( 1000, char_level=True, oov_token=\"<OOV>\") # 맨앞의 1000개의 빈출 단어들만 index로 만들어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d089b1e7-edb7-4d9f-a963-2f428543f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bbab37-76df-4c5e-ba47-8dc9fa3dc772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = TFBertModel.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b27319-6087-4781-b4c5-ff6a3fd1b1de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 128), dtype=tf.int32, name='input_ids'), name='input_ids', description=\"created by layer 'input_ids'\") at layer \"tf_bert_model\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m lstm2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m128\u001b[39m)(lstm1)\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(lstm2)\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:165\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    157\u001b[0m         [\n\u001b[0;32m    158\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    160\u001b[0m         ]\n\u001b[0;32m    161\u001b[0m     ):\n\u001b[0;32m    162\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    163\u001b[0m             inputs, outputs\n\u001b[0;32m    164\u001b[0m         )\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:264\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:1128\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[1;32m-> 1128\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1129\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following previous layers were accessed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m         )\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[0;32m   1135\u001b[0m     computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 128), dtype=tf.int32, name='input_ids'), name='input_ids', description=\"created by layer 'input_ids'\") at layer \"tf_bert_model\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")  # 입력 ID\n",
    "attention_masks = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_masks\")  # 어텐션 마스크\n",
    "\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_masks)[0]  # BERT 출력\n",
    "\n",
    "# 여기서부터는 기존의 LSTM 모델 구조를 이어붙입니다.\n",
    "# 이 예제에서는 BERT 출력의 평균을 가져와 LSTM에 전달하는 방식을 사용합니다.\n",
    "embedding_layer = tf.keras.layers.Embedding(tokenizer.vocab_size + 1, 16)(avg_pool)\n",
    "\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "lstm1 = tf.keras.layers.LSTM(128, return_sequences=True)(embedding_layer)\n",
    "lstm2 = tf.keras.layers.LSTM(128)(lstm1)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(lstm2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca50a76-b765-4952-a056-fffaf3b02030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# 학습 데이터 (trainX, trainY)와 검증 데이터 (valX, valY)를 BERT 입력 형식에 맞게 변환해야 합니다.\n",
    "# 여기서는 이미 변환된 데이터를 사용한다고 가정하였습니다.\n",
    "model.fit(trainX, trainY, validation_data=(valX, valY), epochs=5, batch_size=2**7, verbose=2)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9d655-ccde-4de0-aeaa-875d2e1a26f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa0f41-39e8-45ec-9349-9081a87e2e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
